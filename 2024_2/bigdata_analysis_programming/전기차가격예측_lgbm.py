# -*- coding: utf-8 -*-
"""전기차가격예측_LGBM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w4iu0KM5lBzbdvhtHSLXe6bbj2Sw-WKw

## LightGBM 모델 학습

### encoding
"""

categorical_cols = ['제조사', '모델', '차량상태', '구동방식', '사고이력']

encoder = LabelEncoder()
for col in categorical_cols:
    train_df[col] = encoder.fit_transform(train_df[col].astype(str))
    test_df[col] = encoder.transform(test_df[col].astype(str))

# Random Forest Imputation
def fill_missing_with_random_forest(df, target_col, features):
    df_notnull = df[df[target_col].notnull()]
    df_null = df[df[target_col].isnull()]

    rf_model = RandomForestRegressor(random_state=42, n_estimators=100)
    rf_model.fit(df_notnull[features], df_notnull[target_col])

    predicted_values = rf_model.predict(df_null[features])
    df.loc[df[target_col].isnull(), target_col] = predicted_values

    return df[target_col]

# features
features_for_impute = [ '주행거리(km)', '보증기간(년)', '차량상태']

train_df['배터리용량'] = fill_missing_with_random_forest(
    train_df, '배터리용량', features_for_impute
)
test_df['배터리용량'] = fill_missing_with_random_forest(
    test_df, '배터리용량', features_for_impute
)


# 결측치가 채워진 데이터프레임 표시
print("Updated DataFrame with Imputed Battery Capacity:")
print(train_df.head())
print(train_df.isnull().sum())

# Feature engineering
train_df['주행거리(km)'].replace(0, np.nan, inplace=True)
train_df['가격_주행거리ratio'] = train_df['가격(백만원)'] / train_df['주행거리(km)']
test_df['가격_주행거리ratio'] = 0

"""### scaling"""

numerical_cols = ['배터리용량', '주행거리(km)', '보증기간(년)', '연식(년)', '가격_주행거리ratio']
target = '가격(백만원)'
for col in ['주행거리(km)']:
    train_df[col] = np.log1p(train_df[col])
    test_df[col] = np.log1p(test_df[col])

# Scale features and target
scaler = MinMaxScaler()
train_features = scaler.fit_transform(train_df[categorical_cols + numerical_cols])
test_features = scaler.transform(test_df[categorical_cols + numerical_cols])

target_scaler = MinMaxScaler()
train_target = target_scaler.fit_transform(train_df[[target]])

# Train-test split
X_train, X_val, y_train, y_val = train_test_split(train_features, train_target, test_size=0.2, random_state=42)

"""### LightGBM 모델 학습


"""

#  Optuna
def objective(trial):
    # Suggest hyperparameters
    params = {
        'objective': 'regression',
        'metric': 'rmse',
        'boosting_type': 'gbdt',
        'num_leaves': trial.suggest_int('num_leaves', 31, 63),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.05),
        'max_depth': trial.suggest_int('max_depth', 10, 20),
        'lambda_l1': trial.suggest_float('lambda_l1', 0.0, 1.0),
        'lambda_l2': trial.suggest_float('lambda_l2', 0.0, 1.0),
        'feature_fraction': trial.suggest_float('feature_fraction', 0.7, 0.9),
        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.7, 0.9),
        'bagging_freq': 1,
    }
    # Create LightGBM datasets
    train_data = lgb.Dataset(X_train, label=y_train.ravel())
    val_data = lgb.Dataset(X_val, label=y_val.ravel(), reference=train_data)

    # Manual early stopping
    gbm = lgb.train(
        params,
        train_data,
        valid_sets=[val_data],
        num_boost_round=1000,
        callbacks=[LightGBMPruningCallback(trial, "rmse")],
    )
    y_pred = gbm.predict(X_val)
    return np.sqrt(mean_squared_error(y_val, y_pred))

# Run Optuna optimization
study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=50, n_jobs=-1)

# Best parameters
print("Best Parameters:", study.best_params)
print("Best RMSE:", study.best_value)

# Train final model with best parameters
best_params = study.best_params
train_data = lgb.Dataset(X_train, label=y_train.ravel())
val_data = lgb.Dataset(X_val, label=y_val.ravel(), reference=train_data)

# Train the final model with manual early stopping
num_boost_round = 1000
patience = 50
best_iteration = 0
best_score = float('inf')

for i in range(1, num_boost_round + 1):
    final_model = lgb.train(
        best_params,
        train_data,
        valid_sets=[val_data],
        num_boost_round=i
    )
    y_pred = final_model.predict(X_val, num_iteration=i)
    rmse = np.sqrt(mean_squared_error(y_val, y_pred))
    if i % 10 == 0:
        print(f"==============Iteration {i}, RMSE: {rmse}==============")
    if rmse < best_score:
        best_score = rmse
        best_iteration = i
    elif i - best_iteration >= patience:
        print(f"Early stopping at iteration {best_iteration}")
        break

# Predict on test data
y_pred_scaled = final_model.predict(test_features) #, num_iteration=best_iteration)
y_pred_actual = target_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1))

# Save predictions to submission file
submission = pd.DataFrame({'ID': test_df['ID'], '가격(백만원)': y_pred_actual.flatten()})
submission.to_csv("/content/drive/MyDrive/prog_team/submission_v9.csv", index=False)

print("Submission file saved.")